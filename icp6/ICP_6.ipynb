{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP_6 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ndsdm4yyuQG"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DMZUp82z-5t"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('origin.txt', 'https://www.gutenberg.org/files/1228/1228-0.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOIYrPZa7Olc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52a5cad5-2f0b-4409-e88b-877c372f4856"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 969034 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRUi7s9t7Sgn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "15f2d547-a8a1-4620-daac-b7fb8969d8f0"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿The Project Gutenberg EBook of On the Origin of Species, by Charles Darwin\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and most\r\n",
            "other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever.  You may copy \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ththg4Zs8WMa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8af3c593-5b3a-453e-8702-88762d21be84"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thLWAGiCrPEy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a024635-219f-4d03-a7a9-6f9b94e5cb6c"
      },
      "source": [
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " '\\r',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '°',\n",
              " 'ä',\n",
              " 'æ',\n",
              " 'ë',\n",
              " 'ö',\n",
              " 'ü',\n",
              " '—',\n",
              " '‘',\n",
              " '’',\n",
              " '“',\n",
              " '”',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpeLU4_b8hng"
      },
      "source": [
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdt73lBU8kK4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f7f74137-7754-40b2-bdc0-aadd333783b2"
      },
      "source": [
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\n",
            "T\n",
            "h\n",
            "e\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFIrB9Pfse6i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ba90ad3-82bc-4cde-b364-2269b40494cc"
      },
      "source": [
        "char_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwDmm0QP8ofI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "65cc8766-c4b2-48cf-f287-bc4d7f48bd1f"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\ufeffThe Project Gutenberg EBook of On the Origin of Species, by Charles Darwin\\r\\n\\r\\nThis eBook is for the '\n",
            "'use of anyone anywhere in the United States and most\\r\\nother parts of the world at no cost and with al'\n",
            "'most no restrictions\\r\\nwhatsoever.  You may copy it, give it away or re-use it under the terms of\\r\\nthe'\n",
            "' Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org.  If you are not '\n",
            "\"located in the United States, you'll have\\r\\nto check the laws of the country where you are located bef\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3tKYCnK8tXI"
      },
      "source": [
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv0DfOo68vwR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ab58236-a2cc-4ee5-ca54-4d517c2c7f54"
      },
      "source": [
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  '\\ufeffThe Project Gutenberg EBook of On the Origin of Species, by Charles Darwin\\r\\n\\r\\nThis eBook is for the'\n",
            "Target data: 'The Project Gutenberg EBook of On the Origin of Species, by Charles Darwin\\r\\n\\r\\nThis eBook is for the '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8CtWdFA8yW4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "203fcd6c-8232-4f71-b32c-73cece3735b6"
      },
      "source": [
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 100), (128, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4UoD86W83Wh"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIKBT-af86Kw"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "     tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "     batch_input_shape=[batch_size, None]),\n",
        "     tf.keras.layers.Dropout(0.2),\n",
        "     tf.keras.layers.LSTM(rnn_units,\n",
        "     return_sequences=True,\n",
        "     stateful=True,\n",
        "     recurrent_initializer='glorot_uniform'),\n",
        "     tf.keras.layers.Dropout(0.2), \n",
        "     tf.keras.layers.LSTM(rnn_units,\n",
        "     return_sequences=True,\n",
        "     stateful=True,\n",
        "     recurrent_initializer='glorot_uniform'),\n",
        "     tf.keras.layers.Dropout(0.2),\n",
        "     tf.keras.layers.Dense(300),\n",
        "     tf.keras.layers.Dense(vocab_size)\n",
        " ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjW7QF5r_CzR"
      },
      "source": [
        "\n",
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BVuvoOH_E1R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6309215-c57a-4c12-ae66-38a09c3b8525"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 100, 97) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J1P-tQL_IAQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "f9a7c3f5-4df3-4d95-f96f-22e04c522505"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (128, None, 256)          24832     \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (128, None, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (128, None, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (128, None, 1024)         0         \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (128, None, 1024)         8392704   \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (128, None, 1024)         0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (128, None, 300)          307500    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (128, None, 97)           29197     \n",
            "=================================================================\n",
            "Total params: 14,001,209\n",
            "Trainable params: 14,001,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjUzcPqu_KwB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "83f5e51f-8088-4d3b-8aa7-097fb9767565"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "#This gives us, at each timestep, a prediction of the next character index:\n",
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([80, 55, 89, 79, 13, 86, 79, 51, 16, 75, 60, 32, 63, 90, 91, 58, 68,\n",
              "       94, 65, 31, 62, 80, 58, 79, 36, 43, 88, 58, 42, 40, 60, 23, 82, 59,\n",
              "       35, 39, 52, 43, 36, 92, 26, 51, 57, 96, 22, 53, 78, 58, 87, 70, 28,\n",
              "       70, 61, 18, 52,  5, 31,  5, 53, 38, 86, 20, 32, 52, 96, 30, 79, 50,\n",
              "       78, 21, 20,  4, 54, 72, 31, 50, 55, 66, 96, 93, 31, 19, 95, 16, 74,\n",
              "       28, 32, 14, 47, 87, 64, 27,  5,  6,  8, 89, 61,  3, 57, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtG84z9I_NfB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5010aa50-b72d-49e2-fa15-e5016065719a"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (128, 100, 97)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.573486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MHsa-g7_Qpa"
      },
      "source": [
        "\n",
        "model.compile(optimizer='adam', loss=loss, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMwqzRfT_SgK"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "patience = 10\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hfhr9l_VRy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dceebc48-5bd9-4cae-bd37-a4a827e8ec3d"
      },
      "source": [
        "\n",
        "EPOCHS=50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "74/74 [==============================] - 28s 375ms/step - loss: 3.2684 - accuracy: 0.1423\n",
            "Epoch 2/50\n",
            "74/74 [==============================] - 28s 373ms/step - loss: 2.6367 - accuracy: 0.2669\n",
            "Epoch 3/50\n",
            "74/74 [==============================] - 28s 376ms/step - loss: 2.0976 - accuracy: 0.3939\n",
            "Epoch 4/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 1.7303 - accuracy: 0.4985\n",
            "Epoch 5/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 1.4842 - accuracy: 0.5656\n",
            "Epoch 6/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 1.3485 - accuracy: 0.6010\n",
            "Epoch 7/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 1.2639 - accuracy: 0.6229\n",
            "Epoch 8/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 1.2076 - accuracy: 0.6379\n",
            "Epoch 9/50\n",
            "74/74 [==============================] - 28s 374ms/step - loss: 1.1596 - accuracy: 0.6516\n",
            "Epoch 10/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 1.1212 - accuracy: 0.6619\n",
            "Epoch 11/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 1.0862 - accuracy: 0.6715\n",
            "Epoch 12/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 1.0563 - accuracy: 0.6798\n",
            "Epoch 13/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 1.0264 - accuracy: 0.6881\n",
            "Epoch 14/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.9998 - accuracy: 0.6955\n",
            "Epoch 15/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.9742 - accuracy: 0.7030\n",
            "Epoch 16/50\n",
            "74/74 [==============================] - 28s 376ms/step - loss: 0.9475 - accuracy: 0.7108\n",
            "Epoch 17/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.9221 - accuracy: 0.7186\n",
            "Epoch 18/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.8967 - accuracy: 0.7258\n",
            "Epoch 19/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.8741 - accuracy: 0.7328\n",
            "Epoch 20/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.8491 - accuracy: 0.7400\n",
            "Epoch 21/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.8256 - accuracy: 0.7475\n",
            "Epoch 22/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.8030 - accuracy: 0.7547\n",
            "Epoch 23/50\n",
            "74/74 [==============================] - 28s 376ms/step - loss: 0.7809 - accuracy: 0.7612\n",
            "Epoch 24/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.7594 - accuracy: 0.7682\n",
            "Epoch 25/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.7383 - accuracy: 0.7754\n",
            "Epoch 26/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.7168 - accuracy: 0.7821\n",
            "Epoch 27/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.6974 - accuracy: 0.7886\n",
            "Epoch 28/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.6786 - accuracy: 0.7948\n",
            "Epoch 29/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.6610 - accuracy: 0.8009\n",
            "Epoch 30/50\n",
            "74/74 [==============================] - 28s 376ms/step - loss: 0.6426 - accuracy: 0.8066\n",
            "Epoch 31/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.6247 - accuracy: 0.8127\n",
            "Epoch 32/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.6089 - accuracy: 0.8178\n",
            "Epoch 33/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.5925 - accuracy: 0.8238\n",
            "Epoch 34/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.5772 - accuracy: 0.8285\n",
            "Epoch 35/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.5650 - accuracy: 0.8330\n",
            "Epoch 36/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.5508 - accuracy: 0.8373\n",
            "Epoch 37/50\n",
            "74/74 [==============================] - 28s 374ms/step - loss: 0.5394 - accuracy: 0.8412\n",
            "Epoch 38/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.5279 - accuracy: 0.8455\n",
            "Epoch 39/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.5165 - accuracy: 0.8489\n",
            "Epoch 40/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.5046 - accuracy: 0.8529\n",
            "Epoch 41/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.4970 - accuracy: 0.8559\n",
            "Epoch 42/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.4851 - accuracy: 0.8596\n",
            "Epoch 43/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.4775 - accuracy: 0.8623\n",
            "Epoch 44/50\n",
            "74/74 [==============================] - 28s 375ms/step - loss: 0.4692 - accuracy: 0.8648\n",
            "Epoch 45/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.4619 - accuracy: 0.8671\n",
            "Epoch 46/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.4532 - accuracy: 0.8698\n",
            "Epoch 47/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.4467 - accuracy: 0.8722\n",
            "Epoch 48/50\n",
            "74/74 [==============================] - 28s 378ms/step - loss: 0.4404 - accuracy: 0.8746\n",
            "Epoch 49/50\n",
            "74/74 [==============================] - 28s 377ms/step - loss: 0.4339 - accuracy: 0.8766\n",
            "Epoch 50/50\n",
            "74/74 [==============================] - 28s 379ms/step - loss: 0.4265 - accuracy: 0.8788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNpsZQTfc4Lq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "0a0524eb-4274-4878-a16f-53fe0345289e"
      },
      "source": [
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (1, None, 256)            24832     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (1, None, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (1, None, 1024)           8392704   \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (1, None, 300)            307500    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (1, None, 97)             29197     \n",
            "=================================================================\n",
            "Total params: 14,001,209\n",
            "Trainable params: 14,001,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8MAIyIqc5ml"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 4\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evsZqgeTc-4l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e18334a-22c4-4705-e4b6-9b43b4ea27b8"
      },
      "source": [
        " \n",
        "print(generate_text(model, start_string=\"evolu\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evolution\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}